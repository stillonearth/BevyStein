{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dqn_agent import Agent\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 16\n",
    "\n",
    "STATE_SIZE = (84, 84, 1)\n",
    "\n",
    "ACTION_MAP = {\n",
    "    0: \"IDLE\",\n",
    "    1: \"TURN_LEFT\",\n",
    "    2: \"TURN_RIGHT\",\n",
    "    3: \"LEFT\",\n",
    "    4: \"RIGHT\",\n",
    "    5: \"FORWARD\",\n",
    "    6: \"BACKWARD\",\n",
    "    7: \"SHOOT\",\n",
    "}\n",
    "ACTION_SIZE = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env.Environment(\"bevystein.exe\", (STATE_SIZE[0], STATE_SIZE[1]), NUM_AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = environment.visual_observations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import VisualQNetwork\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=STATE_SIZE, \n",
    "    action_size=ACTION_SIZE, \n",
    "    seed=0, \n",
    "    double=True, \n",
    "    priority_replay=False,\n",
    "    q_network=VisualQNetwork,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import I\n",
    "\n",
    "\n",
    "def augment_state(frames, actions):\n",
    "    action_t_minus_1, action_t = actions[-1], actions[0]\n",
    "    pix_t_minus_1, pix_t, pix_t_plus_1  = frames[0], frames[1], frames[2]\n",
    "\n",
    "    action_t_minus_1 = np.ones((STATE_SIZE[0], STATE_SIZE[1])) * action_t_minus_1\n",
    "    action_t = np.ones((STATE_SIZE[0], STATE_SIZE[1])) * action_t\n",
    "           \n",
    "    state = np.stack([\n",
    "        pix_t_minus_1, \n",
    "        pix_t,\n",
    "        pix_t_plus_1,\n",
    "    ])\n",
    "    \n",
    "    return state\n",
    "\n",
    "def dqn(n_episodes=100, max_t=10000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=max_t)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "        environment.reset()\n",
    "        \n",
    "        framebuffers = []\n",
    "        action_buffers = []\n",
    "\n",
    "        state = np.array(environment.visual_observations())\n",
    "\n",
    "        # get initial 3-frame observation\n",
    "        for i in range(NUM_AGENTS):\n",
    "            framebuffer = deque(maxlen=3)\n",
    "            action_buffer = deque(maxlen=2)\n",
    "\n",
    "            for _ in range(0, 3):\n",
    "                framebuffer.append(state[i])\n",
    "            for _ in range(0, 2):\n",
    "                action_buffer.append(np.ones(1))            \n",
    "            \n",
    "            framebuffers.append(framebuffer)\n",
    "            action_buffers.append(action_buffer)\n",
    "        \n",
    "\n",
    "        state = []\n",
    "        for i in range(0, NUM_AGENTS):\n",
    "            agent_state = augment_state(list(framebuffers[i]), list(action_buffers[i]))\n",
    "            state.append(agent_state)\n",
    "        state = np.array(state)\n",
    "\n",
    "        scores = [0 for i in range(NUM_AGENTS)]\n",
    "        for t in range(max_t):\n",
    "\n",
    "            actions = []\n",
    "            raw_actions = []\n",
    "            rewards = []\n",
    "            is_terminated = []\n",
    "            for i in range(0, NUM_AGENTS):\n",
    "                action = agent.act(state[i], eps).astype(int)\n",
    "                action_buffers[i].append(action)\n",
    "                actions.append(ACTION_MAP[action])\n",
    "                raw_actions.append(int(action))\n",
    "            \n",
    "            (result, screen) = environment.step(actions)\n",
    "            for i in range(0, NUM_AGENTS):\n",
    "                framebuffers[i].append(np.squeeze(screen[i]))\n",
    "                rewards.append(result[i]['reward'])\n",
    "                is_terminated.append(result[i]['is_terminated'])\n",
    "                next_state = augment_state(list(framebuffers[i]), list(action_buffers[i]))\n",
    "                print(float(rewards[i]))\n",
    "                agent.step(state[i], raw_actions[i], rewards[i], next_state, is_terminated[i])\n",
    "                state[i] = next_state\n",
    "                scores[I] += rewards[i]\n",
    "                if is_terminated[i]: continue \n",
    "\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent.qnetwork_local.state_dict(), \"visual_q_network.pth\")\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(n_episodes):\n",
    "    scores = dqn(n_episodes=n_episodes)\n",
    "    # plot the scores\n",
    "    fig = plt.figure()\n",
    "    fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.str_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ssuro\\git\\BevyStein\\python\\DQN.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_dqn(n_episodes\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\ssuro\\git\\BevyStein\\python\\DQN.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain_dqn\u001b[1;34m(n_episodes)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_dqn\u001b[39m(n_episodes):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     scores \u001b[39m=\u001b[39m dqn(n_episodes\u001b[39m=\u001b[39;49mn_episodes)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# plot the scores\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure()\n",
      "\u001b[1;32mc:\\Users\\ssuro\\git\\BevyStein\\python\\DQN.ipynb Cell 8\u001b[0m in \u001b[0;36mdqn\u001b[1;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m next_state \u001b[39m=\u001b[39m augment_state(\u001b[39mlist\u001b[39m(framebuffers[i]), \u001b[39mlist\u001b[39m(action_buffers[i]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mfloat\u001b[39m(rewards[i]))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m agent\u001b[39m.\u001b[39;49mstep(state[i], actions[i], rewards[i], next_state, is_terminated[i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m state[i] \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ssuro/git/BevyStein/python/DQN.ipynb#W6sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m scores[I] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards[i]\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\git\\BevyStein\\python\\dqn_agent.py:74\u001b[0m, in \u001b[0;36mAgent.step\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     72\u001b[0m     \u001b[39m# If enough samples are available in memory, get random subset and learn\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m BATCH_SIZE:\n\u001b[1;32m---> 74\u001b[0m         experiences, idxs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msample()\n\u001b[0;32m     75\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn(experiences, GAMMA, idxs)\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\git\\BevyStein\\python\\dqn_agent.py:186\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    182\u001b[0m experiences \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory, k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m    184\u001b[0m states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(\n\u001b[0;32m    185\u001b[0m     np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39mstate \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m--> 186\u001b[0m actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(\n\u001b[0;32m    187\u001b[0m     np\u001b[39m.\u001b[39;49mvstack([e\u001b[39m.\u001b[39;49maction \u001b[39mfor\u001b[39;49;00m e \u001b[39min\u001b[39;49;00m experiences \u001b[39mif\u001b[39;49;00m e \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m]))\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    188\u001b[0m rewards \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(\n\u001b[0;32m    189\u001b[0m     np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39mreward \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    190\u001b[0m next_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mvstack(\n\u001b[0;32m    191\u001b[0m     [e\u001b[39m.\u001b[39mnext_state \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.str_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "train_dqn(n_episodes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "196608 / 16 / 3 / 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "84 * 84 * 3 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f04677aed7c6774a637d7ce7b6fc8d80c8b672be82d736b4b31c3119a6d1d35d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
